{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIQVWBmeb0zc"
   },
   "source": [
    "# **Projet MultiMédia M2SID**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1cqRgONb0pj"
   },
   "source": [
    "---\n",
    "**Projet MultiMédia, M2 SID, UT3 Paul Sabatier**  \n",
    "\n",
    "Réalisé par : **Léa Fabriol**, **Valentin Lafargue** & **Théotime Dmitrašinović**.  \n",
    "Date : 23/10/2023 -> 10/11/2023  \n",
    "\n",
    "Il est conseiller de cloner ce dossier dans google colab car le projet a été réalisé à l'aide de google drive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY2Hexq-cDJv"
   },
   "source": [
    "# **Arborescence du répertoire GitHub avec Résumés des fichiers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`README`\n",
    "- `README.ipynb` : NoteBook d'édition du README markdown\n",
    "- `README.md` : Le fichier README en markdown affiché par GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzuQ-94d2Yxr"
   },
   "source": [
    "## **`DOC\\`**  \n",
    "\n",
    "- `Rapport_Projet_NA_DM_M2_2023_2024.pdf` : Rapport de Projet  \n",
    "\n",
    "- `Diapo_Projet_NA_DM_M2_2023_2024.pdf` : Diapo Soutenance  \n",
    "  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOZVAuvMmFlR"
   },
   "source": [
    "## **`Media_Extract\\`**  \n",
    "\n",
    "- `Equilibrage_des_classes_Premiers_Tests.ipynb` : Premiers tests d'équilibrage de la distribution des catégories  \n",
    "  \n",
    "- `Pre-traitement.ipynb` : analyse des données du github, parcours des différentes données dont les fameux fichiers json à utilitée doutable, fichier permettant de récupérer et stocker\n",
    "l'audio des vidéos en mp3 (format qui prend moins de place). (-> parallélisation sur création fichier mp3)  \n",
    "  \n",
    "- `Speech2Text.ipynb` : Fichier utilisant ces fichiers mp3 pour obtenir la transcription de l'audio original avec Whisper Small (-> parallélisation sur transcription)  \n",
    "  \n",
    "- `Video32Frames.ipynb` : Clustering sur les images pour sélectionner les 32 images les plus importantes d'une vidéo : Kmeans avec distance classique et k=32, on prend l'image la plus proche de chaque cluster.  \n",
    "  \n",
    "- `ZeroShot.ipynb` : Unsupervised deep learning sur des données textuelles: essayer de relabéliser les données avec les labels présents dans le papier, approche classification directe intéressante mais pas exploitée.  \n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VnfjPtBmFdh"
   },
   "source": [
    "## **`Embeddings_Extract\\`**  \n",
    "\n",
    "- **`Audio\\`** :  \n",
    "  - `Exploration_AudioDetection.ipynb` : Notebook de recherche de modèles pré-entrainés pour la detection audio  \n",
    "    \n",
    "  - `ExtractAudioEmbeddings_VGGish.ipynb` : Récupération des embeddings Audio avec le modèle pré-entrainé VGGish   \n",
    "    \n",
    "  - `ExtractAudioEmbeddings_YAMNet.ipynb` :  Récupération des embeddings Audio avec le modèle pré-entrainé YAMNet  \n",
    "     \n",
    "  - `inferenceALL.py` : Script python utilisé pour la récupération des embeddings avec YAMNet, récupère le vecteur entier (521) et pas seulement les 10 sons environnants les mieux prédits  \n",
    "    \n",
    "\n",
    "- **`Video\\`** :  \n",
    "  - `Embeddings_res_yolo.ipynb` : Transforme en embeddings les résultats de yolo et applique un classifieur aux embeddings  \n",
    "    \n",
    "  - `use_yolo.ipynb` : Fait tourner l’algo yolo et enregistre les résultats dans un fichier   \n",
    "    \n",
    "  - `detect.py` : Fichier du package yolo modifier pour obtenir les sortie voulues à savoir la proba de l’objet et l’objet détecté pour chaque frame  \n",
    "    \n",
    "  - `ViViT2210.ipynb` : Utilisation du model sur nos 32 images sélectionné avec la méthode développé dans `Video32Frames.ipynb`, on récupère l'embedding qui est la dernière couche du model de classification original, on supprime toute la sortie (sauf ce qui nous intéresse) pour la ram et on recommence\n",
    "   \n",
    "\n",
    "- **`Texte\\`** :  \n",
    "  - `mpnet.ipynb` : Fichier avec embeddings de MPnet et prediction Deep + ML  \n",
    "   \n",
    "\n",
    "\n",
    "  \n",
    "- `..\\Classification\\Text.ipynb` : Extraction des embeddings **et** Classification. Prédictions avec DistilBERT ainsi qu'un Model de ML sur un sentenceBERT classique ; des approches classiques de ML (SVC) sur ces embeddings  \n",
    "\n",
    "   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KzMOjMsmFUd"
   },
   "source": [
    "## **`Classification\\`**  \n",
    "\n",
    "\n",
    "- `ClassifAudio_Premiers_Tests.ipynb` :  Premiers tests de classification audio  \n",
    "  \n",
    "- `Classification_Audio.ipynb` : Classification de toutes les combinaisons de labelisation, Embeddings audio et méthode d'équilibrage  \n",
    "  \n",
    "- `ClassifViViT.ipynb` : Deep learning et ML sur les embeddings de ViViT (qui ont du être aggrée par manque de RAM)  \n",
    "  \n",
    "- `Text.ipynb` : Fichier sur lequel on a les prédictions avec DistilBERT ainsi qu'un Model de ML sur un sentenceBERT classique ; des approches classiques de ML (SVC) sur ces embeddings  \n",
    "  \n",
    "- `TranscriptionClassificationML.ipynb` : ML sur des approches d'embeddings simples : countvectorizer, tf-idf\n",
    "  \n",
    "`..\\Embeddings_Extract\\Video\\Embeddings_res_yolo.ipynb` : Extraction des embeddings **et** Classification. Transforme en embeddings les résultats de yolo et applique un classifieur aux embeddings.    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qrawfb_NmFMm"
   },
   "source": [
    "## **`Multi&Cross_Modal\\`**  \n",
    "\n",
    "- `CrossModal_et_MultiModalntermediaire.ipynb` : Embeddings vers Embeddings (audio(VGG-ish)/texte(MPnet)/video(yolo)) ; prédiction intermédiaire avec Réseau de Neurone plsu complexe (tf.split + tf.concat) qui permet de réduire la dimension des embeddings et de les uniformiser en ayant déjkà pour but la classification.  \n",
    "  \n",
    "- `MultiModalPrécoce.ipynb` : Réunion des embeddings des trois modalités (avant classification)\n",
    "  \n",
    "- `MultiModalTardivet.ipynb` : Réunion des probabilités de prédictions des trois modalités (après classification)\n",
    "  \n",
    "- `DALLE_mini.ipynb` :  Transformation de la description écrite en image, but étant de comparer ensuite ces images au images les plus représentatives de la vidéo  \n",
    "  \n",
    "- `Texte_to_image.ipynb` : Test dans le but d'évaluer la correspondance entre les images et la description écrite d’une vidéo avec le modèle CLIP  \n",
    "   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mhqrSwRvWjL"
   },
   "source": [
    "Les Notebooks suivants comprennent à la fois l'extraction des caractéristiques et la classification.  \n",
    "- `Classification\\Text.ipynb`  \n",
    "- `Embeddings_Extract\\Video\\Embeddings_res_yolo.ipynb`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO8DcHzZorePykXUZ++fp9u",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
