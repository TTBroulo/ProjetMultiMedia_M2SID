{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNqRwQ5YXMIg"
   },
   "source": [
    "---\n",
    "Title : Application_VGGish  \n",
    "Author : Dmitrašinović Théotime  \n",
    "Date : 25/10/2023  \n",
    "**But** :  \n",
    "1. Application du modèle pré entrainné YAMNet pour récupérer les prédictions de son environnant\n",
    "\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA4ArfwqccDw"
   },
   "source": [
    "# YAMNet PreTrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Snmi5HSRbjY"
   },
   "source": [
    "## GitHub Explication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJhWiw1Cgxjk"
   },
   "source": [
    "[Lien GitHub](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet)\n",
    "\n",
    "\n",
    "YAMNet is a pretrained deep net that predicts [521 audio event classes](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv) based on the [AudioSet-YouTube corpus](https://research.google.com/audioset/) , and employing the [Mobilenet_v1](https://arxiv.org/pdf/1704.04861.pdf) depthwise-separable convolution architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWj98AbtBSrh"
   },
   "source": [
    "YAMNet also requires downloading the following data file:\n",
    "\n",
    "[YAMNet model weights](https://storage.googleapis.com/audioset/yamnet.h5) in Keras saved weights in HDF5 format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DddXaFpG3gyu"
   },
   "source": [
    "### Install dependences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQt9_2XcclKc",
    "outputId": "db4b5b6e-6db7-40a5-d37e-416ad3a96802"
   },
   "outputs": [],
   "source": [
    "!pip install numpy resampy tensorflow soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjmW3ftTPICN"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sFIGwe8u3b8g",
    "outputId": "04f017d4-236f-42da-bbd3-860dc4d8979b"
   },
   "outputs": [],
   "source": [
    "# Clone TensorFlow models repo into a 'models' directory.\n",
    "!git clone https://github.com/tensorflow/models.git\n",
    "import os\n",
    "os.chdir(\"models/research/audioset/yamnet/\")\n",
    "# Download data file into same directory as code.\n",
    "!curl -O https://storage.googleapis.com/audioset/yamnet.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwnMUPvzBmSo",
    "outputId": "8af66db1-8c08-4800-d75d-8074b0e4345a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installation ready, let's test it.\n",
    "!python yamnet_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB7xWJKXPQ6S"
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNkkWzU2PUMj"
   },
   "source": [
    "You can run the model over existing soundfiles using `inference.py`:\n",
    "\n",
    "```python inference.py input_sound.wav```  \n",
    "\n",
    "The code will report the top-5 highest-scoring classes averaged over all the frames of the input. You can access greater detail by modifying the example code in inference.py.  \n",
    "\n",
    "See the jupyter notebook `yamnet_visualization.ipynb` for an example of displaying the per-frame model output scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6BWUpfTRyAE"
   },
   "source": [
    "### About the Model\n",
    "\n",
    "The YAMNet code layout is as follows:\n",
    "\n",
    "- `yamnet.py`: Model definition in Keras.\n",
    "- `params.py`: Hyperparameters. You can usefully modify PATCH_HOP_SECONDS.\n",
    "- `features.py`: Audio feature extraction helpers.\n",
    "- `inference.py`: Example code to classify input wav files.\n",
    "- `yamnet_test.py`: Simple test of YAMNet installation\n",
    "- `inferenceALL.py`: Pour récupérer le vecteur entier et pas seulement les 10 premiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elJn5vIvSqoL"
   },
   "source": [
    "### Input: Audio Features\n",
    "\n",
    "See `features.py`.  \n",
    "\n",
    "As with our previous release [VGGish](https://github.com/tensorflow/models/tree/master/research/audioset/vggish), YAMNet was trained with audio features computed as follows:  \n",
    "\n",
    "- All audio is resampled to 16 kHz mono.  \n",
    "- A spectrogram is computed using magnitudes of the Short-Time Fourier Transform with a window size of 25 ms, a window hop of 10 ms, and a periodic Hann window.  \n",
    "- A mel spectrogram is computed by mapping the spectrogram to 64 mel bins covering the range 125-7500 Hz.  \n",
    "- A stabilized log mel spectrogram is computed by applying log(mel-spectrum + 0.001) where the offset is used to avoid taking a logarithm of zero.  \n",
    "- These features are then framed into 50%-overlapping examples of 0.96 seconds, where each example covers 64 mel bands and 96 frames of 10 ms each.  \n",
    "\n",
    "These 96x64 patches are then fed into the Mobilenet_v1 model to yield a 3x2 array of activations for 1024 kernels at the top of the convolution. These are averaged to give a 1024-dimension embedding, then put through a single logistic layer to get the 521 per-class output scores corresponding to the 960 ms input waveform segment. (Because of the window framing, you need at least 975 ms of input waveform to get the first frame of output scores.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck4xKONaSqjD"
   },
   "source": [
    "### Class vocabulary\n",
    "\n",
    "The file `yamnet_class_map.csv` describes the audio event classes associated with each of the 521 outputs of the network. Its format is: `index,mid,display_name`  \n",
    "\n",
    "where `index` is the model output index (0..520), `mid` is the machine identifier for that class (e.g. /m/09x0r), and `display_name` is a human-readable description of the class (e.g. Speech).  \n",
    "\n",
    "The original Audioset data release had 527 classes. This model drops six of them on the recommendation of our Fairness reviewers to avoid potentially offensive mislabelings. We dropped the gendered versions (Male/Female) of Speech and Singing. We also dropped Battle cry and Funny music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYVrz9U3Sqdh"
   },
   "outputs": [],
   "source": [
    "yamnet_classes = yamnet_model.class_names('yamnet_class_map.csv')\n",
    "yamnet_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUD3whHhSqWV"
   },
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmtIKv6DXP-A",
    "outputId": "955ae566-6d7c-41f9-fafd-41f0862942b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-YVdA0fXEeX",
    "outputId": "eadf5762-5bad-4fd3-849e-63ef2ca706de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research/audioset/yamnet\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wq7VmaxAtebP"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_embeddings(all_embe, folder_path, version):\n",
    "  # save all embeddings\n",
    "  with open(folder_path + 'Predictions_'+str(version)+'.pkl', 'wb') as ff:\n",
    "    pickle.dump(all_embe, ff)\n",
    "  # save list of id\n",
    "  with open(folder_path + \"log_\"+str(version)+\".pkl\", \"wb\") as fp:\n",
    "    pickle.dump(list(all_embe.keys()), fp)\n",
    "  # on supprime les anciens fichiers\n",
    "  try:\n",
    "    os.remove(folder_path + 'Predictions_'+str(version-1)+'.pkl')\n",
    "    os.remove(folder_path + \"log_\"+str(version-1)+\".pkl\")\n",
    "  except:\n",
    "    print(\"n'a pas pu delete les anciennes sauvegardes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udai8jRuRk4A",
    "outputId": "490cd14a-53a0-4375-8d2d-8635ac0d3b0e"
   },
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbb515ahRio_"
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "def scinderAudio(AudioPath, fileName, maxi = 4000000):\n",
    "  OutPath = \"/content/gdrive/MyDrive/Projet_Multimedia/download/Audio_Temp/\"\n",
    "  sound = AudioSegment.from_mp3(AudioPath+fileName)\n",
    "  bordures = [i*maxi for i in range(int(np.ceil(len(sound) / maxi)))] + [len(sound)]\n",
    "  for s in range(len(bordures)-1):\n",
    "    sound[bordures[s]:bordures[s+1]].export(OutPath+fileName[:-4]+str(bordures[s+1])+\".mp3\", format=\"mp3\")\n",
    "  print(fileName, \" Scindé en\", len(bordures)-1, \"parties\")\n",
    "  return OutPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXFGtCEvH2H_"
   },
   "outputs": [],
   "source": [
    "# MODIFICATION du fichier inference.py\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import resampy\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import params as yamnet_params\n",
    "import yamnet as yamnet_model\n",
    "\n",
    "import time\n",
    "def main(AudioFolderPath, logs):\n",
    "  # recup des audio files\n",
    "  AudioFileName = [fi for fi in listdir(AudioFolderPath) if isfile(join(AudioFolderPath, fi))]\n",
    "  AudioFilePath = [AudioFolderPath + afn for afn in AudioFileName]\n",
    "\n",
    "  params = yamnet_params.Params()\n",
    "  yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "  yamnet.load_weights('yamnet.h5')\n",
    "  yamnet_classes = yamnet_model.class_names('yamnet_class_map.csv')\n",
    "\n",
    "  all_preds = {}\n",
    "\n",
    "  for f, file_name in tqdm_notebook(enumerate(AudioFilePath), desc=\"Fichiers audios traités\"):\n",
    "    if AudioFileName[f][:-4] in logs:\n",
    "      time.sleep(0.01)\n",
    "    else:\n",
    "\n",
    "      # si fichier trop volumineux\n",
    "      if os.path.getsize(AudioFolderPath + AudioFileName[f]) > 40000000:\n",
    "        OutPath = scinderAudio(AudioFolderPath, AudioFileName[f])\n",
    "        files = [fi for fi in listdir(OutPath) if isfile(join(OutPath, fi))]\n",
    "        embes = []\n",
    "        for afile in files:\n",
    "          # get audio signal\n",
    "          audioPath = OutPath + afile\n",
    "          wav_data , sr = librosa.load(audioPath , sr=16000)\n",
    "          #assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n",
    "          waveform = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n",
    "          waveform = waveform.astype('float32')\n",
    "          # Convert to mono and the sample rate expected by YAMNet.\n",
    "          if len(waveform.shape) > 1:\n",
    "            waveform = np.mean(waveform, axis=1)\n",
    "          if sr != params.sample_rate:\n",
    "            waveform = resampy.resample(waveform, sr, params.sample_rate)\n",
    "          # Predict YAMNet classes.\n",
    "          scores, embeddings, spectrogram = yamnet(waveform)\n",
    "          # Scores is a matrix of (time_frames, num_classes) classifier scores.\n",
    "          # Average them along time to get an overall classifier output for the clip.\n",
    "          prediction = np.mean(scores, axis=0)\n",
    "          # Run the model, check the output.\n",
    "          embes.append(prediction)\n",
    "          os.remove(OutPath+afile)\n",
    "        all_preds[AudioFileName[f][:-4]] = np.max(embes, axis=0)\n",
    "      else:\n",
    "\n",
    "        # Decode the WAV file.\n",
    "        wav_data, sr = sf.read(file_name, dtype=np.int16)\n",
    "        assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n",
    "        waveform = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n",
    "        waveform = waveform.astype('float32')\n",
    "\n",
    "        # Convert to mono and the sample rate expected by YAMNet.\n",
    "        if len(waveform.shape) > 1:\n",
    "          waveform = np.mean(waveform, axis=1)\n",
    "        if sr != params.sample_rate:\n",
    "          waveform = resampy.resample(waveform, sr, params.sample_rate)\n",
    "\n",
    "        # Predict YAMNet classes.\n",
    "        scores, embeddings, spectrogram = yamnet(waveform)\n",
    "        # Scores is a matrix of (time_frames, num_classes) classifier scores.\n",
    "        # Average them along time to get an overall classifier output for the clip.\n",
    "        prediction = np.mean(scores, axis=0)\n",
    "        # Report the highest-scoring classes and their scores.\n",
    "        #top5_i = np.argsort(prediction)[::-1]#[:5]\n",
    "        all_preds[AudioFileName[f][:-4]] = prediction\n",
    "        save_embeddings(all_preds, AudioEmbeddingsPath, f)\n",
    "  return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj3U-DJp2i-x"
   },
   "outputs": [],
   "source": [
    "AudioEmbeddingsPath = \"/content/gdrive/MyDrive/Projet_Multimedia/download/Audio_Embeddings/YAMNet/\"\n",
    "AudioPath = \"/content/gdrive/MyDrive/Projet_Multimedia/download/Audio/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuvWCGKJ2crr",
    "outputId": "bd3888af-b0e8-41f5-e7e8-2256fe6e4337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(AudioEmbeddingsPath + 'log_ok_555.pkl', \"rb\") as fp:\n",
    "    logs1 = pickle.load(fp)\n",
    "with open(AudioEmbeddingsPath + 'log_ok_941.pkl', \"rb\") as fp:\n",
    "    logs2 = pickle.load(fp)\n",
    "with open(AudioEmbeddingsPath + 'log_ok_1429.pkl', \"rb\") as fp:\n",
    "    logs3 = pickle.load(fp)\n",
    "logs = logs1 + logs2 + logs3\n",
    "len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "6ZSmXa1fQHJV",
    "outputId": "f24ffbb3-7554-4791-c2d1-bdce868b7802"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'-z2BgjH_CtIA'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jWXK3-2QgKy"
   },
   "outputs": [],
   "source": [
    "AudioFileName = [f for f in listdir(AudioPath) if isfile(join(AudioPath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "UDlJq9nvQlhu",
    "outputId": "74c7f3ad-eccf-4538-fbcb-0017f81a9d85"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'-1wVXK5FKVO0.mp3'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AudioFileName[942]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359,
     "referenced_widgets": [
      "bff8952825134cecb37221c2fa64b810",
      "2bf56588cfa54df9806716a030bcf3af",
      "ee39c8e14de84d0f9ee3687450c5c78c",
      "c933df56af344c018e309dd2de49c480",
      "b832f67d00c2491bb5e2a5afbefbe043",
      "de4ded4155cb4b1e9fe63fae9c1f1714",
      "2fae021ea8194b559240e4c0c8661ee0",
      "b88dd00df7f74a329478f9125ac67bf7",
      "69978f74eaa44ae9a5a4ebb7cf3d847e",
      "17c746b64b334d66a78983b0799be9b8",
      "95f23b2f56c94c8da41e274ae5aae487"
     ]
    },
    "id": "RvYy5a_Qh7EI",
    "outputId": "9be3bdd1-4bf8-4720-e11d-1249ba3fa454"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff8952825134cecb37221c2fa64b810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fichiers audios traités: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n'a pas pu delete les anciennes sauvegardes\n",
      "-3qaKevyVuS4.mp3  Scindé en 1 parties\n",
      "n'a pas pu delete les anciennes sauvegardes\n",
      "-nd9Cen7REwM.mp3  Scindé en 2 parties\n",
      "n'a pas pu delete les anciennes sauvegardes\n",
      "-e_t6_zCrwz0.mp3  Scindé en 1 parties\n",
      "n'a pas pu delete les anciennes sauvegardes\n",
      "-JAAQ2FnnkX8.mp3  Scindé en 1 parties\n",
      "n'a pas pu delete les anciennes sauvegardes\n",
      "-KsREXvSMe9c.mp3  Scindé en 1 parties\n",
      "n'a pas pu delete les anciennes sauvegardes\n",
      "--jeILZA-hDE.mp3  Scindé en 1 parties\n",
      "n'a pas pu delete les anciennes sauvegardes\n",
      "-euTUvnCixSk.mp3  Scindé en 1 parties\n",
      "n'a pas pu delete les anciennes sauvegardes\n",
      "-L8hM2kbw2Ik.mp3  Scindé en 1 parties\n",
      "n'a pas pu delete les anciennes sauvegardes\n"
     ]
    }
   ],
   "source": [
    "pred = main(AudioPath, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvh0xbTtU68q"
   },
   "source": [
    "## Verification et réunion de toutes les sessions de calculs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbjnUhOFQtJM",
    "outputId": "d449bf1d-259c-48cb-ec7f-f5d4ea0998df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2280"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(AudioEmbeddingsPath + 'log_ok_555.pkl', \"rb\") as fp:\n",
    "    logs1 = pickle.load(fp)\n",
    "with open(AudioEmbeddingsPath + 'log_ok_941.pkl', \"rb\") as fp:\n",
    "    logs2 = pickle.load(fp)\n",
    "with open(AudioEmbeddingsPath + 'log_ok_1429.pkl', \"rb\") as fp:\n",
    "    logs3 = pickle.load(fp)\n",
    "with open(AudioEmbeddingsPath + 'log_ok_2279.pkl', \"rb\") as fp:\n",
    "    logs4 = pickle.load(fp)\n",
    "logs = logs1 + logs2 + logs3 + logs4\n",
    "len(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kj6M9uDtVD0a",
    "outputId": "50f11534-6e69-4119-a161-d3188e14f6ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n'a pas pu delete les anciennes sauvegardes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2280"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(AudioEmbeddingsPath + 'Predictions_ok_555.pkl', \"rb\") as fp:\n",
    "    embe1 = pickle.load(fp)\n",
    "with open(AudioEmbeddingsPath + 'Predictions_ok_941.pkl', \"rb\") as fp:\n",
    "    embe2 = pickle.load(fp)\n",
    "with open(AudioEmbeddingsPath + 'Predictions_ok_1429.pkl', \"rb\") as fp:\n",
    "    embe3 = pickle.load(fp)\n",
    "with open(AudioEmbeddingsPath + 'Predictions_ok_2279.pkl', \"rb\") as fp:\n",
    "    embe4 = pickle.load(fp)\n",
    "embes = {**embe1, **embe2, **embe3, **embe4}\n",
    "\n",
    "save_embeddings(embes, AudioEmbeddingsPath, \"ALL_predictions_YAMNet\")\n",
    "len(embes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFn6o9xCVDpG"
   },
   "outputs": [],
   "source": [
    "with open(AudioEmbeddingsPath + 'ALL_predictions_YAMNet.pkl', \"rb\") as fp:\n",
    "    embes = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hg010MnWLvJ",
    "outputId": "e10b16d8-7ff3-4cc0-f4ba-b7d447fe4abf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2280"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embes)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "PN2B1vURWhed",
    "XmbXHSPcVA70"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "17c746b64b334d66a78983b0799be9b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bf56588cfa54df9806716a030bcf3af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de4ded4155cb4b1e9fe63fae9c1f1714",
      "placeholder": "​",
      "style": "IPY_MODEL_2fae021ea8194b559240e4c0c8661ee0",
      "value": "Fichiers audios traités: "
     }
    },
    "2fae021ea8194b559240e4c0c8661ee0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69978f74eaa44ae9a5a4ebb7cf3d847e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "95f23b2f56c94c8da41e274ae5aae487": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b832f67d00c2491bb5e2a5afbefbe043": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b88dd00df7f74a329478f9125ac67bf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "bff8952825134cecb37221c2fa64b810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2bf56588cfa54df9806716a030bcf3af",
       "IPY_MODEL_ee39c8e14de84d0f9ee3687450c5c78c",
       "IPY_MODEL_c933df56af344c018e309dd2de49c480"
      ],
      "layout": "IPY_MODEL_b832f67d00c2491bb5e2a5afbefbe043"
     }
    },
    "c933df56af344c018e309dd2de49c480": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17c746b64b334d66a78983b0799be9b8",
      "placeholder": "​",
      "style": "IPY_MODEL_95f23b2f56c94c8da41e274ae5aae487",
      "value": " 2280/? [2:27:28&lt;00:00,  8.25s/it]"
     }
    },
    "de4ded4155cb4b1e9fe63fae9c1f1714": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee39c8e14de84d0f9ee3687450c5c78c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b88dd00df7f74a329478f9125ac67bf7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69978f74eaa44ae9a5a4ebb7cf3d847e",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
