{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Utiliser résultats yolo"
      ],
      "metadata": {
        "id": "ihbcgk57w0Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports et installation"
      ],
      "metadata": {
        "id": "Zqwf8j6Vw5vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3xkjGv1raUR",
        "outputId": "ec2ee86a-fb2c-4314-fb6c-f37b44ef9bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=f0bad66ca7c1929890719d3ca2325a17ec00a3279d31a6215b784df517e9cc27\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AdamW\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Reshape,Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,UpSampling2D,InputLayer,ReLU,BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD,Adam, AdamW\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import scipy.ndimage\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9CSFR66npEiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Récupération des données depuis le drive"
      ],
      "metadata": {
        "id": "W-SyEcyZxEr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7ODGDlVpVrB",
        "outputId": "cc7f5e06-1839-47d3-c169-9d12c08dd751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {}\n",
        "for i in range(20):\n",
        " path = \"/content/gdrive/MyDrive/Projet_Multimedia/download/yolo_json/objets_videos_mod_\" + str(i) + \".json\"\n",
        " file = open(path)\n",
        " with open(path, 'r') as f:\n",
        "  dictionnary = json.load(f)\n",
        " dic = dic | dictionnary\n",
        "len(dic)\n",
        "\n",
        "for i in range(2):\n",
        " path = \"/content/gdrive/MyDrive/Projet_Multimedia/download/yolo_json/objets_videos_rest_0\"+str(i)+\".json\"\n",
        " file = open(path)\n",
        " with open(path, 'r') as f:\n",
        "  dictionnary = json.load(f)\n",
        " dic = dic | dictionnary\n",
        "path = \"/content/gdrive/MyDrive/Projet_Multimedia/download/yolo_json/objets_videos_rest_final.json\"\n",
        "file = open(path)\n",
        "with open(path, 'r') as f:\n",
        " dictionnary = json.load(f)\n",
        "dic = dic | dictionnary\n",
        "len(dic)\n",
        "\n",
        "# récup les labels\n",
        "path = \"/content/gdrive/MyDrive/Projet_Multimedia/download/CSV/NLP.csv\"\n",
        "df_lab = pd.read_csv(path)\n",
        "df = df_lab"
      ],
      "metadata": {
        "id": "B8Tw8y1DpOOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings avec BERT"
      ],
      "metadata": {
        "id": "Qpp8Z2v1xbPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recupérer les mots\n",
        "dic_obj = {}\n",
        "dic_obj_u = {}\n",
        "\n",
        "for video,frames in dic.items() :\n",
        "  objets_concat = ''\n",
        "  objets_concat_unique = ''\n",
        "  for frame, mots in frames.items() :\n",
        "    for mot, proba in mots.items() :\n",
        "      m = mot + ' '\n",
        "      objets_concat += m\n",
        "      if (mot not in objets_concat_unique) and (proba > 0.5):\n",
        "        objets_concat_unique += m\n",
        "\n",
        "  dic_obj[video] = objets_concat\n",
        "  dic_obj_u[video] = objets_concat_unique\n",
        "\n",
        "# inclure dans le df les colonnes contenant les mots concat et concats uniques\n",
        "def f(x) :\n",
        "  try :\n",
        "    obj = dic_obj['video'+x+'.mp4']\n",
        "  except :\n",
        "    obj = '0'\n",
        "  return obj\n",
        "\n",
        "def f_u(x) :\n",
        "  try :\n",
        "    obj = dic_obj_u['video'+x+'.mp4']\n",
        "  except :\n",
        "    obj = '0'\n",
        "  return obj\n",
        "\n",
        "df['objets_concat'] = df['name'].apply(lambda x : f(x))\n",
        "df['objets_concat_unique'] = df['name'].apply(lambda x : f_u(x))\n",
        "\n",
        "df = df[df.new_label != 'not done yet']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l1tA3IRpuid",
        "outputId": "7124c45e-f880-4f26-956d-7f3abab075ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-300-6088f3adba40>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['objets_concat'] = df['name'].apply(lambda x : f(x))\n",
            "<ipython-input-300-6088f3adba40>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['objets_concat_unique'] = df['name'].apply(lambda x : f_u(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer les train et test\n",
        "df['label'] = df['label'].astype('category')\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['objets_concat_unique'], df['label'], test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
        "\n",
        "# Charger le modèle Sentence-Transformers\n",
        "sentencebert_model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "# Encoder les textes avec Sentence-Transformers\n",
        "train_embeddings = sentencebert_model.encode(X_train.tolist(), convert_to_tensor=True)\n",
        "test_embeddings = sentencebert_model.encode(X_test.tolist(), convert_to_tensor=True)\n",
        "valid_embeddings = sentencebert_model.encode(X_val.tolist(), convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "OjWflOGTqnju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_embeddings.cpu().data.numpy()\n",
        "X_test = test_embeddings.cpu().data.numpy()\n",
        "X_val = valid_embeddings.cpu().data.numpy()"
      ],
      "metadata": {
        "id": "iDUMNCjhtMTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDTPYVHMpDRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30d38c8-f1c4-4402-e04a-b1088af223cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4129353233830846\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40049751243781095"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ],
      "source": [
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf.fit(X_train,y_train)\n",
        "tfsvmpred = clf.predict(X_test)\n",
        "print(accuracy_score(y_test,tfsvmpred))\n",
        "\n",
        "\n",
        "predovr = OneVsRestClassifier(SVC(random_state=0)).fit(X_train, y_train).predict(X_test)\n",
        "accuracy_score(y_test,predovr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pti_reseau_clas(size_in,size_out):\n",
        "\n",
        "  x_in = Input(size_in)\n",
        "  x = x_in\n",
        "\n",
        "  layers = [256, 256, 64]\n",
        "\n",
        "  for c in layers :\n",
        "    x = Dense(c)(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "  x_out = Dense(size_out, activation='softmax')(x)\n",
        "\n",
        "  model = Model(inputs = x_in, outputs = x_out)\n",
        "  opt = AdamW()\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "\n",
        "  return model\n",
        "\n",
        "model=pti_reseau_clas(size_in = X_train.shape[1],size_out=df.label.nunique())\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHFj8TWToqbu",
        "outputId": "8226433d-de11-431b-9fee-31bdbb039e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_48 (InputLayer)       [(None, 768)]             0         \n",
            "                                                                 \n",
            " dense_140 (Dense)           (None, 256)               196864    \n",
            "                                                                 \n",
            " re_lu_94 (ReLU)             (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_77 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_141 (Dense)           (None, 256)               65792     \n",
            "                                                                 \n",
            " re_lu_95 (ReLU)             (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_78 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_142 (Dense)           (None, 64)                16448     \n",
            "                                                                 \n",
            " re_lu_96 (ReLU)             (None, 64)                0         \n",
            "                                                                 \n",
            " dropout_79 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_143 (Dense)           (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 280079 (1.07 MB)\n",
            "Trainable params: 280079 (1.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs=15\n",
        "hist = model.fit(X_train, pd.get_dummies(y_train),\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, pd.get_dummies(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ou1W4IHo9x0",
        "outputId": "fc89b83d-328d-4e2f-b5e7-3a7f30ac3fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "10/10 [==============================] - 3s 29ms/step - loss: 2.5324 - accuracy: 0.2114 - val_loss: 2.2433 - val_accuracy: 0.2935\n",
            "Epoch 2/15\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 2.3924 - accuracy: 0.2629 - val_loss: 2.2265 - val_accuracy: 0.3184\n",
            "Epoch 3/15\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 2.2829 - accuracy: 0.2761 - val_loss: 2.1535 - val_accuracy: 0.3159\n",
            "Epoch 4/15\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.1975 - accuracy: 0.3002 - val_loss: 2.1133 - val_accuracy: 0.3284\n",
            "Epoch 5/15\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.1677 - accuracy: 0.2910 - val_loss: 2.1107 - val_accuracy: 0.3383\n",
            "Epoch 6/15\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.1432 - accuracy: 0.3027 - val_loss: 2.0543 - val_accuracy: 0.3483\n",
            "Epoch 7/15\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 2.1191 - accuracy: 0.3159 - val_loss: 2.0483 - val_accuracy: 0.3980\n",
            "Epoch 8/15\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 2.1108 - accuracy: 0.3226 - val_loss: 2.0224 - val_accuracy: 0.3856\n",
            "Epoch 9/15\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 2.0700 - accuracy: 0.3267 - val_loss: 2.0157 - val_accuracy: 0.3930\n",
            "Epoch 10/15\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 2.0370 - accuracy: 0.3425 - val_loss: 1.9858 - val_accuracy: 0.3930\n",
            "Epoch 11/15\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 2.0298 - accuracy: 0.3425 - val_loss: 1.9541 - val_accuracy: 0.4154\n",
            "Epoch 12/15\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 1.9726 - accuracy: 0.3607 - val_loss: 1.9521 - val_accuracy: 0.4005\n",
            "Epoch 13/15\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.9417 - accuracy: 0.3682 - val_loss: 1.9440 - val_accuracy: 0.4030\n",
            "Epoch 14/15\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.9372 - accuracy: 0.3698 - val_loss: 1.9519 - val_accuracy: 0.4129\n",
            "Epoch 15/15\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 1.8981 - accuracy: 0.3831 - val_loss: 1.9300 - val_accuracy: 0.4005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(X_val, pd.get_dummies(y_val),  verbose=0)\n",
        "print('The accuracy on the test set is ',(acc*100),'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rkkHY79wG4b",
        "outputId": "583430a5-3018-4cb1-df3e-ed403adc19bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on the test set is  38.05970251560211 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(y_val)\n",
        "ind = Y[Y.Education == 0].reset_index().index\n",
        "x_val_woe = X_val[ind]"
      ],
      "metadata": {
        "id": "7DRmxG8zsk9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(x_val_woe, Y[Y.Education == 0],  verbose=0)\n",
        "print('The accuracy on the test set is ',(acc*100),'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tin8S6scpy36",
        "outputId": "4f905ec3-e3f5-4ffb-fd5a-b76ec9f7b278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on the test set is  5.198776721954346 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "liste = np.argmax(model.predict(X_train), axis = 1)\n",
        "dic = {}\n",
        "for elem in liste:\n",
        "  if elem in dic:\n",
        "    dic[elem] +=1\n",
        "  else:\n",
        "    dic[elem] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPYIak6YtyNs",
        "outputId": "17754c44-37d2-4ec8-acdd-e0f24dfc1127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RTd484Vu6Lc",
        "outputId": "8af9e6ab-b79e-4b05-bb62-0687d0778451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2: 917, 4: 213, 3: 92, 6: 110, 10: 35}"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(Y, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XqRxaHat-Yn",
        "outputId": "9ba979eb-7684-4ad0-ffd1-4b8f5fbed6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Autos & Vehicles           4\n",
              "Comedy                    16\n",
              "Education                129\n",
              "Entertainment             63\n",
              "Film & Animation          71\n",
              "Gaming                     6\n",
              "Howto & Style             28\n",
              "Music                     13\n",
              "News & Politics           17\n",
              "Nonprofits & Activism     17\n",
              "People & Blogs            60\n",
              "Pets & Animals             8\n",
              "Science & Technology      11\n",
              "Sports                     6\n",
              "Travel & Events            7\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings à la mano"
      ],
      "metadata": {
        "id": "RBOP3xIkxjSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str(dic)[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "nXuDjZFjxoMa",
        "outputId": "10cb9f08-643a-4ba7-fbc6-a0c4e9f777e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"{'video-AZS5cgybKcI.mp4': {'94': {'car': 0.44}, '95': {'car': 0.33}, '96': {'car': 0.43}, '97': {'truck': 0.35, 'car': 0.61}, '98': {'traffic light': 0.26, 'person': 0.37, 'car': 0.77}, '99': {'traffi\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# indexer les mots\n",
        "dic_obj = {}\n",
        "index = 0\n",
        "for video,frames in dic.items() :\n",
        "  for frame, mots in frames.items() :\n",
        "    for mot, proba in mots.items() :\n",
        "      if (mot not in dic_obj):\n",
        "        dic_obj[mot] = index\n",
        "        index += 1"
      ],
      "metadata": {
        "id": "0TR0aLlRyqWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_name(nom):\n",
        "  return nom[5:-4]\n",
        "crop_name('video-AZS5cgybKcI.mp4')"
      ],
      "metadata": {
        "id": "bl68ifrEzVcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy as copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# pour chaque video on initialise la liste d'apparition de chaque mots\n",
        "dic_obj_full_0 = {}\n",
        "for mot in dic_obj.keys():\n",
        "  dic_obj_full_0 [mot] = [0]\n",
        "\n",
        "# création du dico dans lequel se trouve la proba max de chaque mot pour chaque video\n",
        "dic_prob_mot = {}\n",
        "for video, frames in tqdm(dic.items()) :\n",
        "  copy_dic_obj_full_0 = copy.copy(dic_obj_full_0)\n",
        "  for frames, mots in frames.items() :\n",
        "    for mot, proba in mots.items():\n",
        "      copy_dic_obj_full_0[mot].append(proba)\n",
        "\n",
        "  dic_max = {}\n",
        "  for mot in copy_dic_obj_full_0.keys() :\n",
        "    dic_max[mot] = max(copy_dic_obj_full_0[mot])\n",
        "\n",
        "  dic_prob_mot[crop_name(video)] = dic_max"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9HLLG4NLsvB",
        "outputId": "26c5ac41-6af6-4916-ceeb-933ce62fdb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2279/2279 [04:05<00:00,  9.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy as copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# pour chaque video on initialise la liste d'apparition de chaque mots\n",
        "dic_obj_full_0 = {}\n",
        "for mot in dic_obj.keys():\n",
        "  dic_obj_full_0 [mot] = [0]\n",
        "\n",
        "# création du dico dans lequel se trouve la proba max de chaque mot pour chaque video\n",
        "dic_avg_prob_mot = {}\n",
        "for video, frames in tqdm(dic.items()) :\n",
        "  copy_dic_obj_full_0 = copy.copy(dic_obj_full_0)\n",
        "  for frames, mots in frames.items() :\n",
        "    for mot, proba in mots.items():\n",
        "      copy_dic_obj_full_0[mot].append(proba)\n",
        "\n",
        "  dic_avg = {}\n",
        "  for mot in copy_dic_obj_full_0.keys() :\n",
        "    # valeur moyenne\n",
        "    dic_avg[mot] = sum(copy_dic_obj_full_0[mot])/len(copy_dic_obj_full_0[mot])\n",
        "\n",
        "\n",
        "  dic_avg_prob_mot[crop_name(video)] = dic_avg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FITwmPMFZ813",
        "outputId": "03e636c3-fdde-4672-9b5f-788aaff38b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2279/2279 [02:11<00:00, 17.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_avg_prob_mot['-KHQk1_Vq69E']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SF_pvnHR7jX",
        "outputId": "2d0af587-f4d4-4687-8b49-dd9cc95f6876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'car': 0.5021428571428571,\n",
              " 'truck': 0.31521739130434784,\n",
              " 'traffic light': 0.43059701492537306,\n",
              " 'person': 0.5221948717948717,\n",
              " 'bus': 0.6763636363636363,\n",
              " 'tv': 0.34643874643874656,\n",
              " 'cat': 0.4702660753880261,\n",
              " 'skateboard': 0.365,\n",
              " 'bottle': 0.46142857142857135,\n",
              " 'dog': 0.5802080443828023,\n",
              " 'vase': 0.35951219512195115,\n",
              " 'cup': 0.35580645161290325,\n",
              " 'fire hydrant': 0.4539999999999999,\n",
              " 'snowboard': 0.2,\n",
              " 'cake': 0.3325,\n",
              " 'train': 0.3295238095238095,\n",
              " 'airplane': 0.3014285714285715,\n",
              " 'bench': 0.34944444444444445,\n",
              " 'backpack': 0.336923076923077,\n",
              " 'chair': 0.48311475409836047,\n",
              " 'dining table': 0.30428571428571427,\n",
              " 'potted plant': 0.305,\n",
              " 'teddy bear': 0.38494117647058823,\n",
              " 'bird': 0.4120588235294118,\n",
              " 'bicycle': 0.7539473684210529,\n",
              " 'tennis racket': 0.329,\n",
              " 'horse': 0.4567961165048543,\n",
              " 'cow': 0.45551724137931027,\n",
              " 'couch': 0.24142857142857146,\n",
              " 'bed': 0.145,\n",
              " 'bowl': 0.165,\n",
              " 'mouse': 0.28700000000000003,\n",
              " 'sheep': 0.4700000000000001,\n",
              " 'cell phone': 0.19666666666666668,\n",
              " 'laptop': 0.4,\n",
              " 'keyboard': 0.26142857142857145,\n",
              " 'umbrella': 0.31,\n",
              " 'book': 0.5570000000000002,\n",
              " 'orange': 0.4683333333333333,\n",
              " 'sports ball': 0.6900000000000001,\n",
              " 'motorcycle': 0.3724324324324325,\n",
              " 'boat': 0.3928333333333332,\n",
              " 'handbag': 0.30034482758620684,\n",
              " 'kite': 0.29,\n",
              " 'bear': 0.23,\n",
              " 'remote': 0.5116666666666666,\n",
              " 'baseball bat': 0.15,\n",
              " 'giraffe': 0.23,\n",
              " 'elephant': 0.20333333333333334,\n",
              " 'toilet': 0.2771428571428572,\n",
              " 'wine glass': 0.125,\n",
              " 'sink': 0.135,\n",
              " 'tie': 0.0,\n",
              " 'banana': 0.0,\n",
              " 'microwave': 0.0,\n",
              " 'refrigerator': 0.0,\n",
              " 'pizza': 0.0,\n",
              " 'stop sign': 0.0,\n",
              " 'frisbee': 0.0,\n",
              " 'suitcase': 0.0,\n",
              " 'clock': 0.0,\n",
              " 'skis': 0.0,\n",
              " 'sandwich': 0.0,\n",
              " 'donut': 0.0,\n",
              " 'apple': 0.0,\n",
              " 'hot dog': 0.0,\n",
              " 'toothbrush': 0.0,\n",
              " 'oven': 0.0,\n",
              " 'baseball glove': 0.0,\n",
              " 'zebra': 0.0,\n",
              " 'scissors': 0.0,\n",
              " 'knife': 0.0,\n",
              " 'surfboard': 0.0,\n",
              " 'parking meter': 0.0,\n",
              " 'broccoli': 0.0,\n",
              " 'carrot': 0.0,\n",
              " 'fork': 0.0,\n",
              " 'spoon': 0.0,\n",
              " 'toaster': 0.0,\n",
              " 'hair drier': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df[df['name'] == '-_YoeHOTJBI4'].index)"
      ],
      "metadata": {
        "id": "J50ZA5BTUAG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def save_embeddings(all_embe, folder_path, version):\n",
        "  # save all embeddings\n",
        "  with open(folder_path + 'all_embeddings_'+version+'.pkl', 'wb') as ff:\n",
        "    pickle.dump(all_embe, ff)\n",
        "\n",
        "def getEmbesDF(path):\n",
        "  with open(path, \"rb\") as fp:\n",
        "      embeddings = pickle.load(fp)\n",
        "  df_embes = pd.DataFrame.from_dict((embeddings), orient='index')\n",
        "  return df_embes"
      ],
      "metadata": {
        "id": "z5ZRVpRlzkou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FolderOUT = \"/content/gdrive/MyDrive/Projet_Multimedia/download/Video_Embeddings/Video_Embeddings/\"\n",
        "save_embeddings(dic_prob_mot, FolderOUT, \"maxProba\")\n",
        "save_embeddings(dic_avg_prob_mot, FolderOUT, \"avgProba\")"
      ],
      "metadata": {
        "id": "xNjtuea4znS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_YOLO_avgProba = getEmbesDF(FolderOUT + \"all_embeddings_avgProba.pkl\")\n",
        "df2 = df[['name', 'label']]\n",
        "good = df2.set_index('name').join(df_YOLO_avgProba)"
      ],
      "metadata": {
        "id": "jWaeqe_Gz4j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good"
      ],
      "metadata": {
        "id": "JKAdH17j0A6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# créer les embeddings\n",
        "embeddings = []\n",
        "def add_embedding(x):\n",
        "  embeddings.append([0]*len(dic_obj))\n",
        "  for mot, index in dic_obj.items():\n",
        "      embeddings[-1][index] = dic_prob_mot['video' + x + '.mp4'][mot]\n",
        "\n",
        "df['name'].apply(lambda x : add_embedding(x))\n",
        "embeddings = np.array(embeddings)"
      ],
      "metadata": {
        "id": "hrWahAV5EY2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test de modèles sur les embeddings"
      ],
      "metadata": {
        "id": "9wVQJCzVYzpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer les train et test\n",
        "df['label'] = df['label'].astype('category')\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, df['label'], test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "2Dz_pgZYX4zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVC"
      ],
      "metadata": {
        "id": "Nzhlu17ZZFsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf.fit(X_train,y_train)\n",
        "tfsvmpred = clf.predict(X_test)\n",
        "accuracy_score(y_test,tfsvmpred)\n",
        "\n",
        "\n",
        "predovr = OneVsRestClassifier(SVC(random_state=0)).fit(X_train, y_train).predict(X_test)\n",
        "accuracy_score(y_test,predovr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7X3y6zCYcQ2",
        "outputId": "9995c1b5-5912-4860-b561-0a60c2ed830c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11403508771929824"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Réseau pas très profond"
      ],
      "metadata": {
        "id": "WptjBn7zbEJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pti_reseau_clas(size_in,size_out):\n",
        "\n",
        "  x_in = Input(size_in)\n",
        "  x = x_in\n",
        "\n",
        "  layers = [256,256,64]\n",
        "\n",
        "  for c in layers :\n",
        "    x = Dense(c)(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "  x_out = Dense(size_out, activation='softmax')(x)\n",
        "\n",
        "  model = Model(inputs = x_in, outputs = x_out)\n",
        "  opt = AdamW()\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
        "\n",
        "  return model\n",
        "\n",
        "model=pti_reseau_clas(size_in = len(dic_obj),size_out=df.label.nunique())\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGJrgN0abDP3",
        "outputId": "3970e03c-9c3f-4122-8997-418c6d42e3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_45 (InputLayer)       [(None, 80)]              0         \n",
            "                                                                 \n",
            " dense_128 (Dense)           (None, 256)               20736     \n",
            "                                                                 \n",
            " re_lu_85 (ReLU)             (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_68 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_129 (Dense)           (None, 256)               65792     \n",
            "                                                                 \n",
            " re_lu_86 (ReLU)             (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_69 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_130 (Dense)           (None, 64)                16448     \n",
            "                                                                 \n",
            " re_lu_87 (ReLU)             (None, 64)                0         \n",
            "                                                                 \n",
            " dropout_70 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_131 (Dense)           (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 103951 (406.06 KB)\n",
            "Trainable params: 103951 (406.06 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs=20\n",
        "hist = model.fit(X_train, pd.get_dummies(y_train),\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, pd.get_dummies(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE-NGBbueI-B",
        "outputId": "68db74c6-0763-489e-e5d8-77c429a639b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "6/6 [==============================] - 4s 137ms/step - loss: 2.6265 - accuracy: 0.1880 - val_loss: 2.3624 - val_accuracy: 0.2829\n",
            "Epoch 2/20\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 2.4180 - accuracy: 0.3058 - val_loss: 2.2961 - val_accuracy: 0.2829\n",
            "Epoch 3/20\n",
            "6/6 [==============================] - 0s 20ms/step - loss: 2.3510 - accuracy: 0.2970 - val_loss: 2.2977 - val_accuracy: 0.2829\n",
            "Epoch 4/20\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 2.3357 - accuracy: 0.2992 - val_loss: 2.2683 - val_accuracy: 0.2829\n",
            "Epoch 5/20\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 2.3040 - accuracy: 0.2985 - val_loss: 2.2282 - val_accuracy: 0.2829\n",
            "Epoch 6/20\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 2.2618 - accuracy: 0.3197 - val_loss: 2.2451 - val_accuracy: 0.2829\n",
            "Epoch 7/20\n",
            "6/6 [==============================] - 0s 21ms/step - loss: 2.2545 - accuracy: 0.3219 - val_loss: 2.2351 - val_accuracy: 0.2829\n",
            "Epoch 8/20\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 2.2324 - accuracy: 0.3292 - val_loss: 2.2161 - val_accuracy: 0.2829\n",
            "Epoch 9/20\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 2.2155 - accuracy: 0.3226 - val_loss: 2.2162 - val_accuracy: 0.2829\n",
            "Epoch 10/20\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 2.2161 - accuracy: 0.3226 - val_loss: 2.2077 - val_accuracy: 0.2829\n",
            "Epoch 11/20\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 2.2158 - accuracy: 0.3124 - val_loss: 2.2055 - val_accuracy: 0.2829\n",
            "Epoch 12/20\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 2.1997 - accuracy: 0.3109 - val_loss: 2.1994 - val_accuracy: 0.2829\n",
            "Epoch 13/20\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 2.2154 - accuracy: 0.3285 - val_loss: 2.1992 - val_accuracy: 0.2829\n",
            "Epoch 14/20\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 2.1808 - accuracy: 0.3314 - val_loss: 2.1981 - val_accuracy: 0.2829\n",
            "Epoch 15/20\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 2.1832 - accuracy: 0.3402 - val_loss: 2.1863 - val_accuracy: 0.2829\n",
            "Epoch 16/20\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 2.1819 - accuracy: 0.3358 - val_loss: 2.1885 - val_accuracy: 0.2829\n",
            "Epoch 17/20\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 2.1882 - accuracy: 0.3328 - val_loss: 2.2033 - val_accuracy: 0.2829\n",
            "Epoch 18/20\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 2.1864 - accuracy: 0.3248 - val_loss: 2.1907 - val_accuracy: 0.2829\n",
            "Epoch 19/20\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 2.1786 - accuracy: 0.3365 - val_loss: 2.1896 - val_accuracy: 0.2829\n",
            "Epoch 20/20\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 2.1774 - accuracy: 0.3277 - val_loss: 2.1979 - val_accuracy: 0.2829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(X_val, pd.get_dummies(y_val),  verbose=0)\n",
        "print('The accuracy on the test set is ',(acc*100),'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMg3EclVjj8F",
        "outputId": "38907934-9f11-4293-c8c5-f4f22cc061ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy on the test set is  28.289473056793213 %\n"
          ]
        }
      ]
    }
  ]
}